{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: evaluate your sentiment classifier\n",
    "It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "- Do any of your classifiers seem to overfit?\n",
    "- Which seem to perform the best? Why?\n",
    "- Which features seemed to be most impactful to performance?\n",
    "- Write up your iterations and answers to the above questions in a few pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment_analysis_challenge/amazon_cells_labelled.txt', sep='\\t', header=None, names=['review','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "data = vectorizer.fit_transform(df.review)\n",
    "target = df.positive\n",
    "mnb = MultinomialNB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=.3)\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "(y_pred == y_test).sum()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So there's 117 True Negatives, 132 True Positives, 33 False Positives and 18 False Negatives\n"
     ]
    }
   ],
   "source": [
    "c_mat = confusion_matrix(y_test, y_pred)\n",
    "TN = c_mat[0][0]\n",
    "TP = c_mat[1][1]\n",
    "FP = c_mat[0][1]\n",
    "FN = c_mat[1][0]\n",
    "TOT = y_test.shape[0]\n",
    "print('So there\\'s {} True Negatives, {} True Positives, {} False Positives and {} False Negatives'.format(TN, TP, FP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 80.0%\n",
      "Specificity: 86.67%\n"
     ]
    }
   ],
   "source": [
    "print('Sensitivity: {}%\\nSpecificity: {}%'.format(round(TP/(TP+FP)*100, 2), round(TN/(TN+FN)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82, 0.85, 0.85, 0.8 , 0.83, 0.8 , 0.78, 0.8 , 0.85, 0.79])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that the multinomial naive Bayes method works decently well for this data set when it comes to cross validation. A standard guess the majority would only garner 50% correct. While splitting things into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1847"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 1847 words in this feature list. That is quite a lot! Wonder if it can predict the Yelp and IMDB better than the IMDB version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('sentiment_analysis_challenge/yelp_labelled.txt', sep='\\t', header=None, names=['review','positive'])\n",
    "imdb = pd.read_csv('sentiment_analysis_challenge/imdb_labelled.txt', sep='\\t', header=None, names=['review','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = vectorizer.transform(yelp.review)\n",
    "i_data = vectorizer.transform(imdb.review)\n",
    "y_target = yelp.positive\n",
    "i_target = imdb.positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724\n",
      "0.6737967914438503\n"
     ]
    }
   ],
   "source": [
    "yelp_pred = mnb.predict(y_data)\n",
    "imdb_pred = mnb.predict(i_data)\n",
    "print((yelp_pred == y_target).sum()/y_target.shape[0])\n",
    "print((imdb_pred == i_target).sum()/i_target.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does look like it performs a bit better than the IMDB version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(actual, prediction):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual, prediction)\n",
    "    print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.7240000000000001\n",
      "0.6764663784959779\n"
     ]
    }
   ],
   "source": [
    "get_auc(y_test, y_pred)\n",
    "get_auc(y_target, yelp_pred)\n",
    "get_auc(i_target, imdb_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

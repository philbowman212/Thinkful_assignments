{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion = gutenberg.raw(fileids='austen-persuasion.txt')\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "print(alice[:100]) #first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "# Print the first 100 characters of Alice again.\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter headings removed:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n"
     ]
    }
   ],
   "source": [
    "# Now we'll match and remove chapter headings.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# Ok, what's it look like now?\n",
    "print('Chapter headings removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "# Remove newlines and other extra whitespace by splitting and rejoining.\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# All done with cleanup? Let's see how it looks.\n",
    "print('Extra whitespace removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34408 tokens long\n",
      "The first three tokens are 'Alice was beginning'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('the', 1524), ('and', 796), ('to', 724), ('a', 611), ('I', 533), ('it', 524), ('she', 508), ('of', 499), ('said', 453), ('Alice', 394)]\n",
      "Persuasion: [('the', 3120), ('to', 2775), ('and', 2738), ('of', 2563), ('a', 1529), ('in', 1346), ('was', 1329), ('had', 1177), ('her', 1159), ('I', 1118)]\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "alice_freq = word_frequencies(alice_doc).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('said', 453), ('Alice', 394), ('little', 124), ('like', 84), ('went', 83), ('know', 83), ('thought', 74), ('Queen', 73), ('time', 68), ('King', 61)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 254), ('Wentworth', 217), ('Lady', 191), ('good', 181), ('little', 175), ('Charles', 166)]\n"
     ]
    }
   ],
   "source": [
    "# Use our optional keyword argument to remove stop words.\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice unqiues: {'said', 'like', 'went', 'Alice', 'Queen', 'thought', 'time', 'King', 'know'}\n",
      "Persuasion uniques: {'Captain', 'Wentworth', 'Mr', 'Elliot', 'Lady', 'Anne', 'Charles', 'Mrs', 'good'}\n"
     ]
    }
   ],
   "source": [
    "print('Alice unqiues: {}'.format(set(alice_common) - set(persuasion_common)))\n",
    "print('Persuasion uniques: {}'.format(set(persuasion_common) - set(alice_common)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('say', 476), ('Alice', 394), ('think', 130), ('go', 130), ('little', 124), ('look', 105), ('know', 103), ('come', 96), ('like', 92), ('begin', 91)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('think', 258), ('Mr', 254), ('know', 252), ('good', 222), ('Wentworth', 215), ('Lady', 191)]\n",
      "Unique to Alice: {'go', 'begin', 'like', 'look', 'Alice', 'say', 'little', 'come'}\n",
      "Unique to Persuasion: {'Captain', 'Wentworth', 'Mr', 'Elliot', 'Lady', 'Anne', 'Mrs', 'good'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('Persuasion:', persuasion_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1989 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences.\n",
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 words in this sentence, and 25 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence.\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON ('nsubj', 'be')\n",
      "shall VERB ('aux', 'be')\n",
      "be AUX ('ROOT', 'be')\n",
      "late ADJ ('acomp', 'be')\n",
      "! PUNCT ('punct', 'be')\n",
      "' PUNCT ('punct', 'be')\n"
     ]
    }
   ],
   "source": [
    "for token in sentences[4]:\n",
    "    print(token, token.pos_, (token.dep_, token.head.orth_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Alice\n",
      "PERSON Alice\n",
      "ORG White Rabbit\n",
      "PERSON Alice\n",
      "ORG WAISTCOAT - POCKET\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "ORDINAL First\n",
      "WORK_OF_ART ' ORANGE MARMALADE '\n",
      "CARDINAL one\n",
      "PERSON Alice\n",
      "QUANTITY four thousand miles\n",
      "PERSON Alice\n",
      "PERSON Latitude\n",
      "PERSON Longitude\n",
      "PERSON Alice\n",
      "PERSON Latitude\n",
      "PERSON Longitude\n",
      "WORK_OF_ART Antipathies\n"
     ]
    }
   ],
   "source": [
    "for entity in list(alice_doc.ents)[0:20]:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adventures', 'Seaography', 'Mine', '--it', 'Mabel', 'Somebody', 'Frog', 'Mouse', 'Owl', 'Alice', 'Cat', 'Cheshire Puss', 'Edwin', 'Fury', 'William the Conqueror', 'Mary Ann', 'Lobster', 'Dinah', 'Shark', 'Knave', 'William', 'Bill', 'Hatter', 'riddles.--I', 'Alice aloud', 'Soo', 'Curiouser', 'Latin Grammar', 'then!--Bill', 'Jack', 'Boots', 'Sha', 'Tillie', 'Latitude', 'Morcar', 'Pat', 'ALICE', 'the Queen of Hearts', 'Miss', 'Beau', 'Shakespeare', 'Queen', 'Swim', 'Wonderland', 'Lizard', 'Duchess', 'Lacie', 'Edgar Atheling', 'Herald', 'Run', \"I'LL\", 'to--', 'Ou', 'Gryphon', 'Rabbit', 'WILLIAM', 'Tut', 'Laughing', 'hippopotamus', 'Mercia', 'Longitude', \"Dinah'll\", 'Grief', 'Geography', 'Lory', 'Ma', 'Treacle', 'Footman', 'Pray', 'Dodo'}\n"
     ]
    }
   ],
   "source": [
    "# All of the uniqe entities spaCy thinks are people.\n",
    "people = [entity.text for entity in list(alice_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = ['a','b','c','d']\n",
    "set(a + b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
